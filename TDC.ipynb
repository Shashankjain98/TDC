import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np

# Define the number of GPUsa
num_gpus = 4

# Create a MirroredStrategy for multi-GPU training strategy
strategy = tf.distribute.MirroredStrategy(devices=["/gpu:0", "/gpu:1", "/gpu:2", "/gpu:3"])
print(f"Number of devices: {strategy.num_replicas_in_sync}")

# Define a simple neural network model
def create_model():
    model = keras.Sequential([
        layers.Input(shape=(784,)),
        layers.Dense(128, activation='relu'),
        layers.Dense(10, activation='softmax')
    ])
    model.compile(loss="sparse_categorical_crossentropy", optimizer="adam", metrics=['accuracy'])
    return model

# Create a distributed model
with strategy.scope():
    distributed_model = create_model()

# Generate synthetic data for training
(x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0  # Normalize pixel values

# Reshape the data
x_train = x_train.reshape(x_train.shape[0], -1)
x_test = x_test.reshape(x_test.shape[0], -1)

# Define batch size
batch_size = 64

# Create datasets for training and testing
train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(60000).batch(batch_size)
test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)

# Training loop
epochs = 5
for epoch in range(epochs):
    total_loss = 0.0
    num_batches = 0

    for x, y in train_dataset:
        num_batches += 1
        with strategy.scope():
            loss_value = distributed_model.train_on_batch(x, y)
        total_loss += loss_value[0]

    average_loss = total_loss / num_batches
    print(f"Epoch {epoch + 1}, Loss: {average_loss:.4f}")

# Evaluate the model
test_loss, test_accuracy = distributed_model.evaluate(test_dataset)





Output:
Number of devices: 4




The provided TensorFlow code showcases distributed training of a neural network using the MirroredStrategy, designed for multi-GPU environments. 
The program defines a simple neural network, compiles it, and creates a distributed model within the specified strategy. 
Using the Fashion MNIST dataset, the code preprocesses and reshapes the data, sets up datasets, and enters a training loop over multiple epochs. 
The model is evaluated on a test dataset, and the average loss is printed for each epoch. 
The focus is on demonstrating distributed training with TensorFlow, emphasizing the utilization of multiple GPUs for improved performance.
