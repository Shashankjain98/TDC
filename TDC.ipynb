{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "If9QDyJX9hGs"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "*Define the number of GPUs\n",
        "\n",
        "num gpus = 4\n",
        "\n",
        "Create a MirroredStrategy for multi-GPU training strategy tf.distribute.MirroredStrategy(devices[\"/gpu:0\", \"/gpu:1\", \"7gpu:2\", \"/gpu:3\"])\n",
        "\n",
        "print(\"Number of devices: [strategy.num replicas in synce)')\n",
        "\n",
        "* Define a simple neural network model def create_model(): model = keras. Sequential(\n",
        "\n",
        "layers. Input (shape=(784,)),\n",
        "\n",
        "layers.Dense (128, activations relu),\n",
        "\n",
        "layers. Dense (10, activations softmax)\n",
        "\n",
        "model.compile(loss=\"sparse_categorical_crossentropy, optimizers\"adam\", metrics=['accuracy\"]} return model\n",
        "\n",
        "#Create a distributed model\n",
        "\n",
        "with strategy.scope():\n",
        "\n",
        "distributed model = create_model()\n",
        "\n",
        "Generate synthetic data for training\n",
        "\n",
        "(x_train, y_train), (_test, y_test) keras.datasets.anist.load_data() x_train, x_testx_train/ 255.0, x_test/255.0 # Normalize pisel values\n",
        "\n",
        "Reshape the data\n",
        "\n",
        "x_train_train.reshape(x_train.shape[0], -1) testx_test.reshape(x_test.shape[e], 1)\n",
        "\n",
        "Define batch size batch_size=64\n",
        "\n",
        "#Create datasets for training and testing train_dataset tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(60000).batch(batch_size) test_dataset = tf.data.Dataset.from tensor slices((x_test, y_test)).batch batch_size)\n",
        "\n",
        "#Training loop epochs S\n",
        "\n",
        "for epoch in range(epochs): total loss 0.0\n",
        "\n",
        "nun batches-0\n",
        "\n",
        "for x, y in train dataset\n",
        "\n",
        "num batches = @\n",
        "\n",
        "for x, y in train_dataset: with strategy.scope():\n",
        "\n",
        "loss_value = distributed_model.train_on_batch(x, y) num_batches += 1\n",
        "\n",
        "total loss loss_value[0]\n",
        "\n",
        "average_loss = total loss / num_batches\n",
        "\n",
        "print(\"Epoch (epoch + 1), Loss: (average_loss:.4f}\")\n",
        "\n",
        "#Evaluate the model\n",
        "\n",
        "test_loss, test_accuracy distributed_model.evaluate(test_dataset)\n",
        "\n",
        "print(\"Test Loss: (test loss: 4f), Test Accuracy: (test accuracy:.2%)\")"
      ]
    }
  ]
}