import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np

# Define the number of GPUs
num_gpus = 4

# Create a MirroredStrategy for multi-GPU training strategy
strategy = tf.distribute.MirroredStrategy(devices=["/gpu:0", "/gpu:1", "/gpu:2", "/gpu:3"])
print(f"Number of devices: {strategy.num_replicas_in_sync}")

# Define a simple neural network model
def create_model():
    model = keras.Sequential([
        layers.Input(shape=(784,)),
        layers.Dense(128, activation='relu'),
        layers.Dense(10, activation='softmax')
    ])
    model.compile(loss="sparse_categorical_crossentropy", optimizer="adam", metrics=['accuracy'])
    return model

# Create a distributed model
with strategy.scope():
    distributed_model = create_model()

# Generate synthetic data for training
(x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0  # Normalize pixel values

# Reshape the data
x_train = x_train.reshape(x_train.shape[0], -1)
x_test = x_test.reshape(x_test.shape[0], -1)

# Define batch size
batch_size = 64

# Create datasets for training and testing
train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(60000).batch(batch_size)
test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)

# Training loop
epochs = 5
for epoch in range(epochs):
    total_loss = 0.0
    num_batches = 0
    
    for x, y in train_dataset:
        num_batches += 1
        with strategy.scope():
            loss_value = distributed_model.train_on_batch(x, y)
        total_loss += loss_value[0]
    
    average_loss = total_loss / num_batches
    print(f"Epoch {epoch + 1}, Loss: {average_loss:.4f}")

# Evaluate the model
test_loss, test_accuracy = distributed_model.evaluate(test_dataset)
print(f"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy * 100:.2f}%")
