{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Distributed Deep Learning with TensorFlow: Simple Multi-GPU Training Example**"
      ],
      "metadata": {
        "id": "9kEgz-LGdRH4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5oF3fDXwV6yD",
        "outputId": "7a5badc0-b8cb-4a3e-e8d2-c4132ca21998"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n",
            "Number of devices: 1\n",
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 13s 6ms/step - loss: 0.3016 - accuracy: 0.9124\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.1471 - accuracy: 0.9561\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.1096 - accuracy: 0.9663\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 10s 6ms/step - loss: 0.0880 - accuracy: 0.9725\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0738 - accuracy: 0.9770\n",
            "313/313 - 1s - loss: 0.0759 - accuracy: 0.9769 - 1s/epoch - 4ms/step\n",
            "\n",
            "Test accuracy: 0.9768999814987183\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Define a simple model\n",
        "def create_model():\n",
        "    model = models.Sequential([\n",
        "        layers.Flatten(input_shape=(28, 28)),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.Dropout(0.2),\n",
        "        layers.Dense(10)\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Load dataset (for example, the MNIST dataset)\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "# Preprocess the data\n",
        "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
        "\n",
        "# Create a strategy for distributed training\n",
        "strategy = tf.distribute.MirroredStrategy()\n",
        "\n",
        "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
        "\n",
        "# Open a strategy scope.\n",
        "with strategy.scope():\n",
        "    # Create and compile the model within the strategy scope\n",
        "    model = create_model()\n",
        "\n",
        "# Train the model using the fit method\n",
        "model.fit(train_images, train_labels, epochs=5)\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\n",
        "print('\\nTest accuracy:', test_acc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The provided code showcases a simple implementation of distributed deep learning using TensorFlow, specifically for multi-GPU training. The script defines a basic neural network using the Keras API, preprocesses the MNIST dataset, and employs the MirroredStrategy for synchronous training across multiple GPUs. Within the strategy scope, the model is created and compiled, and then trained using the fit method. Finally, the script evaluates the trained model on the test data, printing the test accuracy. The overall goal is to demonstrate a straightforward example of leveraging distributed computing for enhanced efficiency in deep learning tasks."
      ],
      "metadata": {
        "id": "EgvE4jsTdxZO"
      }
    }
  ]
}